{"cells":[{"cell_type":"markdown","metadata":{"id":"qkGy7XGmir1H"},"source":["**Running OntoZSL on ImNet-A with Basic KG in the Standard ZSL setting**\n","---\n","You can run other settings by changing the parameters of \"dataset\", \"manual_seed\", \"batch_size\", \"lr\", \"noise_size\" and \"semantic type\".\n","\n","The parameters in other settings are attached in the end.\n"]},{"cell_type":"markdown","source":["**1. Bind your Google Drive**"],"metadata":{"id":"AxW-jy3smlVN"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4180,"status":"ok","timestamp":1666189337711,"user":{"displayName":"黄雨峰","userId":"10355830370640481145"},"user_tz":-480},"id":"JMil2TBwSYDI","outputId":"15d48abe-9997-40c5-9de6-0b566e3c3b1f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"markdown","source":["**2. Import Package**"],"metadata":{"id":"inJpLv0dmu6y"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"S3bhQTvbfU6y","executionInfo":{"status":"ok","timestamp":1666189343250,"user_tz":-480,"elapsed":2613,"user":{"displayName":"黄雨峰","userId":"10355830370640481145"}}},"outputs":[],"source":["import os\n","import sys\n","import time\n","import random\n","import argparse\n","import scipy.io as scio\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.autograd as autograd\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","from sklearn import preprocessing\n","from sklearn.preprocessing import MinMaxScaler\n"]},{"cell_type":"markdown","source":["**3. Data Preparation and Loading**"],"metadata":{"id":"SEw72dThm8ot"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"yxgGS_os7T_Q","executionInfo":{"status":"ok","timestamp":1666189348904,"user_tz":-480,"elapsed":722,"user":{"displayName":"黄雨峰","userId":"10355830370640481145"}}},"outputs":[],"source":["def load_semantic_embed(data_path, dataset, type):\n","    \"\"\"\n","    Load Semantic Embeddings.\n","    \"\"\"\n","\n","    file_name = ''\n","\n","    if dataset == 'AwA2':\n","        file_path = os.path.join(data_path, 'semantic_embeddings')\n","        if type == 'att':\n","            file_name = os.path.join(data_path, 'binaryAtt_splits.mat')\n","        elif type == 'w2v':\n","            file_name = os.path.join(file_path, 'awa_w2v.mat')\n","        elif type == 'w2v-glove':\n","            file_name = os.path.join(file_path, 'awa_w2v_glove.mat')\n","        elif type == 'hie':\n","            file_name = os.path.join(file_path, 'awa_hierarchy_gae.mat')\n","        elif type == 'kge':\n","            file_name = os.path.join(file_path, 'kge_CH_AH_CA_60000.mat')\n","        elif type == 'kge_text':\n","            file_name = os.path.join(file_path, 'kge_CH_AH_CA_60000_text_140.mat')\n","        elif type == 'kge_facts':\n","            file_name = os.path.join(file_path, 'kge_CH_AH_CA_Facts_60000_80000.mat')\n","        elif type == 'kge_logics':\n","            file_name = os.path.join(file_path, 'kge_CH_AH_CA_Logics_70000.mat')\n","        else:\n","            print(\"WARNING: invalid semantic embeddings type\")\n","\n","    else:\n","        file_path = os.path.join(data_path, dataset, 'semantic_embeddings')\n","        if type == 'hie':\n","            file_name = os.path.join(file_path, 'hierarchy_gae.mat')\n","        elif type == 'w2v':\n","            file_name = os.path.join(data_path, 'w2v.mat')\n","        elif type == 'w2v-glove':\n","            file_name = os.path.join(file_path, 'w2v_glove.mat')\n","        elif type == 'att':\n","            file_name = os.path.join(file_path, 'atts_binary.mat')\n","        else:\n","            print('WARNING: invalid semantic embeddings type')\n","\n","\n","\n","        if dataset == 'ImNet_A':\n","            if type == 'kge':\n","                file_name = os.path.join(file_path, 'kge_CH_AH_CA_60000.mat')\n","            elif type == 'kge_text':\n","                file_name = os.path.join(file_path, 'kge_CH_AH_CA_60000_text_nei_140.mat')\n","            elif type == 'kge_facts':\n","                file_name = os.path.join(file_path, 'kge_CH_AH_CA_Facts_60000_70000.mat')\n","        if dataset == 'ImNet_O':\n","            if type == 'kge':\n","                file_name = os.path.join(file_path, 'kge_CH_AH_CA_60000.mat')\n","            elif type == 'kge_text':\n","                file_name = os.path.join(file_path, 'kge_CH_AH_CA_60000_text_nei_140.mat')\n","            elif type == 'kge_facts':\n","                file_name = os.path.join(file_path, 'kge_CH_AH_CA_Facts_60000_70000.mat')\n","\n","\n","    if file_name:\n","        matcontent = scio.loadmat(file_name)\n","        if dataset == 'AwA2':\n","            if type == 'att':\n","                embeddings = matcontent['att'].T\n","            else:\n","                embeddings = matcontent['embeddings']\n","        else:\n","            if type == 'w2v':\n","                embeddings = matcontent['w2v'][:2549]\n","            else:\n","                embeddings = matcontent['embeddings']\n","    else:\n","        print('WARNING: invalid semantic embeddings file path')\n","    return embeddings\n","\n","def load_imagenet(data_path, dataset):\n","\n","    def load_classes(file_name):\n","        classes = list()\n","        wnids = open(file_name, 'rU')\n","        try:\n","            for line in wnids:\n","                classes.append(line[:-1])\n","        finally:\n","            wnids.close()\n","        return classes\n","\n","    def read_features(file_path, inds, type, nsample=None):\n","        fea_set = list()\n","        label_set = list()\n","        for idx in inds:\n","            # print(idx)\n","            file = os.path.join(file_path, str(idx)+'.mat')\n","            feature = np.array(scio.loadmat(file)['features'])\n","            if type == 'seen':\n","                if nsample and feature.shape[0] > nsample:\n","                    feature = feature[:nsample]\n","            label = np.array((idx-1), dtype=int)\n","            label = label.repeat(feature.shape[0])\n","            fea_set.append(feature)\n","            label_set.append(label)\n","\n","        return np.vstack(tuple(fea_set)), np.hstack(tuple(label_set))\n","\n","    # split.mat : wnids, words\n","    matcontent = scio.loadmat(os.path.join(data_path, 'split.mat'))\n","    wnids = matcontent['allwnids'].squeeze().tolist()\n","\n","\n","    seen_classes = load_classes(os.path.join(data_path, dataset, 'seen.txt'))\n","    unseen_classes = load_classes(os.path.join(data_path, dataset, 'unseen.txt'))\n","    seen_index = [wnids.index(wnid)+1 for wnid in seen_classes]\n","    unseen_index = [wnids.index(wnid) + 1 for wnid in unseen_classes]\n","\n","\n","    train_seen_feat_file = os.path.join(data_path, 'Res101_Features', 'ILSVRC2012_train')\n","    test_seen_feat_file = os.path.join(data_path, 'Res101_Features', 'ILSVRC2012_val')\n","    test_unseen_feat_file = os.path.join(data_path, 'Res101_Features', 'ILSVRC2011')\n","\n","    train_seen_features, train_seen_labels = read_features(train_seen_feat_file, seen_index, 'seen')\n","    # extract a subset with 300 images per classes for training classifier\n","    train_seen_features_sub, train_seen_labels_sub = read_features(train_seen_feat_file, seen_index, 'seen', 300)\n","    test_unseen_features, test_unseen_labels = read_features(test_unseen_feat_file, unseen_index, 'unseen')\n","    test_seen_features, test_seen_labels = read_features(test_seen_feat_file, seen_index, 'seen')\n","\n","    return train_seen_features, train_seen_labels, \\\n","           train_seen_features_sub, train_seen_labels_sub, \\\n","           test_unseen_features, test_unseen_labels, \\\n","           test_seen_features, test_seen_labels\n","\n","\n","def load_dataset(data_path):\n","    # load resnet features\n","    matcontent = scio.loadmat(os.path.join(data_path, 'res101.mat'))\n","    feature = matcontent['features'].T\n","    label = matcontent['labels'].astype(int).squeeze() - 1\n","\n","    # load split.mat\n","    split_matcontent = scio.loadmat(os.path.join(data_path, 'binaryAtt_splits.mat'))\n","    # numpy array index starts from 0, matlab starts from 1\n","    trainval_loc = split_matcontent['trainval_loc'].squeeze() - 1\n","    test_seen_loc = split_matcontent['test_seen_loc'].squeeze() - 1\n","    test_unseen_loc = split_matcontent['test_unseen_loc'].squeeze() - 1\n","\n","    return feature[trainval_loc], label[trainval_loc], \\\n","           feature[test_unseen_loc], label[test_unseen_loc], \\\n","           feature[test_seen_loc], label[test_seen_loc]\n","\n","\n","def GetNowTime():\n","    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time()))\n","\n","def map_label(label, classes):\n","    mapped_label = torch.LongTensor(label.size())  # 19832\n","    for i in range(classes.size(0)):\n","        mapped_label[label == classes[i]] = i\n","    return mapped_label\n","\n","class DATA_LOADER(object):\n","    def __init__(self, args):\n","\n","        if args.dataset == 'AwA2':\n","            self.read_dataset(args)\n","        else:\n","            self.read_imagenet(args)\n","\n","        self.index_in_epoch = 0\n","        self.epochs_completed = 0\n","\n","        self.feat_dim = self.train_seen_feature.shape[1]  # 2048\n","        self.sem_dim = self.semantic.shape[1]  # 500\n","\n","        self.ntrain = self.train_seen_feature.size()[0]  # number of training samples\n","\n","        self.seenclasses = torch.from_numpy(np.unique(self.train_seen_label.numpy()))\n","        self.unseenclasses = torch.from_numpy(np.unique(self.test_unseen_label.numpy()))\n","\n","\n","\n","\n","    def read_imagenet(self, args):\n","        data_path = os.path.join(args.data_dir, 'ImageNet')\n","\n","        # read seen features\n","        train_seen_features, train_seen_labels,\\\n","            train_seen_features_sub, train_seen_labels_sub, \\\n","            test_unseen_features, test_unseen_labels, \\\n","            test_seen_features, test_seen_labels = load_imagenet(data_path, args.dataset)\n","\n","        scaler = preprocessing.MinMaxScaler()\n","        self.train_seen_feature = torch.from_numpy(scaler.fit_transform(train_seen_features)).float()\n","        self.train_seen_label = torch.from_numpy(train_seen_labels).long()\n","\n","        self.test_unseen_feature = torch.from_numpy(scaler.transform(test_unseen_features)).float()\n","        self.test_unseen_label = torch.from_numpy(test_unseen_labels).long()\n","\n","        self.test_seen_feature = torch.from_numpy(scaler.transform(test_seen_features)).float()\n","        self.test_seen_label = torch.from_numpy(test_seen_labels).long()\n","\n","        # self.train_seen_feature_sub = torch.from_numpy(scaler.fit_transform(train_seen_features_sub)).float()\n","        # self.train_seen_label_sub = torch.from_numpy(train_seen_labels_sub).long()\n","\n","        self.train_seen_feature_sub = torch.from_numpy(train_seen_features_sub).float()\n","        self.train_seen_label_sub = torch.from_numpy(train_seen_labels_sub).long()\n","\n","\n","        embeddings = load_semantic_embed(data_path, args.dataset, args.semantic_type)\n","        self.semantic = torch.from_numpy(embeddings).float()\n","\n","\n","    def read_dataset(self, args):\n","        data_path = os.path.join(args.data_dir, args.dataset)\n","        # read seen features\n","        train_seen_features, train_seen_labels, \\\n","        test_unseen_features, test_unseen_labels, \\\n","        test_seen_features, test_seen_labels = load_dataset(data_path)\n","\n","        # if args.pre_process:\n","        scaler = preprocessing.MinMaxScaler()\n","        self.train_seen_feature = torch.from_numpy(scaler.fit_transform(train_seen_features)).float()\n","        self.train_seen_label = torch.from_numpy(train_seen_labels).long()\n","        mx = self.train_seen_feature.max()\n","        self.train_seen_feature.mul_(1 / mx)\n","\n","        self.test_unseen_feature = torch.from_numpy(scaler.fit_transform(test_unseen_features)).float()\n","        self.test_unseen_feature.mul_(1 / mx)\n","        self.test_unseen_label = torch.from_numpy(test_unseen_labels).long()\n","\n","        self.test_seen_feature = torch.from_numpy(scaler.fit_transform(test_seen_features)).float()\n","        self.test_seen_feature.mul_(1 / mx)\n","        self.test_seen_label = torch.from_numpy(test_seen_labels).long()\n","\n","        embeddings = load_semantic_embed(data_path, args.dataset, args.semantic_type)\n","        self.semantic = torch.from_numpy(embeddings).float()\n","\n","\n","\n","\n","    def next_batch_one_class(self, batch_size):\n","        if self.index_in_epoch == self.ntrain_class:\n","            self.index_in_epoch = 0\n","            perm = torch.randperm(self.ntrain_class)\n","            self.train_class[perm] = self.train_class[perm]\n","\n","        iclass = self.train_class[self.index_in_epoch]\n","        idx = self.train_seen_label.eq(iclass).nonzero().squeeze()\n","        perm = torch.randperm(idx.size(0))\n","        idx = idx[perm]\n","        iclass_feature = self.train_seen_feature[idx]\n","        iclass_label = self.train_seen_label[idx]\n","        self.index_in_epoch += 1\n","        return iclass_feature[0:batch_size], iclass_label[0:batch_size], self.semantic[iclass_label[0:batch_size]]\n","\n","    def next_batch(self, batch_size):\n","        idx = torch.randperm(self.ntrain)[0:batch_size]\n","        batch_feature = self.train_seen_feature[idx]\n","        batch_label = self.train_seen_label[idx]\n","        batch_sem = self.semantic[batch_label]\n","        return batch_feature, batch_label, batch_sem\n","\n","    # select batch samples by randomly drawing batch_size classes\n","    def next_batch_uniform_class(self, batch_size):\n","        batch_class = torch.LongTensor(batch_size)\n","        for i in range(batch_size):\n","            idx = torch.randperm(self.ntrain_class)[0]\n","            batch_class[i] = self.train_class[idx]\n","\n","        batch_feature = torch.FloatTensor(batch_size, self.train_seen_feature.size(1))\n","        batch_label = torch.LongTensor(batch_size)\n","        batch_sem = torch.FloatTensor(batch_size, self.semantic.size(1))\n","        for i in range(batch_size):\n","            iclass = batch_class[i]\n","            idx_iclass = self.train_seen_label.eq(iclass).nonzero().squeeze()\n","            idx_in_iclass = torch.randperm(idx_iclass.size(0))[0]\n","            idx_file = idx_iclass[idx_in_iclass]\n","            batch_feature[i] = self.train_seen_feature[idx_file]\n","            batch_label[i] = self.train_seen_label[idx_file]\n","            batch_sem[i] = self.semantic[batch_label[i]]\n","        return batch_feature, batch_label, batch_sem"]},{"cell_type":"markdown","source":["**4. Classifier in testing stage, is trained with generated unseen (and real seen) features**"],"metadata":{"id":"E_ibaUUPnHmH"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"U_dS5TF37WS_","executionInfo":{"status":"ok","timestamp":1666189361536,"user_tz":-480,"elapsed":1192,"user":{"displayName":"黄雨峰","userId":"10355830370640481145"}}},"outputs":[],"source":["class CLASSIFIER:\n","    # train_Y is interger\n","    # CLASSIFIER(syn_feature,util.map_label(syn_label,data.unseenclasses),data,data.unseenclasses.size(0),opt.cuda,opt.classifier_lr, 0.5, 25, opt.syn_num, False)\n","    def __init__(self, args, _train_X, _train_Y, data_loader, _nclass, _cuda, _lr=0.001, _beta1=0.5, _nepoch=20,\n","                 _batch_size=100, generalized=True, ratio=0.6, epoch=20):\n","\n","        self.train_X = _train_X\n","        self.train_Y = _train_Y\n","        self.args = args\n","\n","        self.test_seen_feature = data_loader.test_seen_feature\n","        self.test_seen_label = data_loader.test_seen_label\n","        self.test_unseen_feature = data_loader.test_unseen_feature\n","        self.test_unseen_label = data_loader.test_unseen_label\n","        self.seenclasses = data_loader.seenclasses\n","        self.unseenclasses = data_loader.unseenclasses\n","        self.batch_size = _batch_size\n","        self.nepoch = _nepoch\n","        self.nclass = _nclass\n","        self.input_dim = _train_X.size(1)\n","        self.cuda = _cuda\n","        self.model = LINEAR_LOGSOFTMAX(self.input_dim, self.nclass)\n","        self.model.apply(weights_init)\n","        self.criterion = nn.NLLLoss()\n","\n","\n","        self.data = data_loader\n","\n","        self.input = torch.FloatTensor(_batch_size, self.input_dim)\n","        self.label = torch.LongTensor(_batch_size)\n","\n","        self.lr = _lr\n","        self.beta1 = _beta1\n","        # setup optimizer\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=_lr, betas=(_beta1, 0.999))\n","        self.ratio = ratio\n","        self.epoch = epoch\n","\n","        if self.cuda:\n","            self.model.cuda()\n","            self.criterion.cuda()\n","            self.input = self.input.cuda()\n","            self.label = self.label.cuda()\n","\n","        self.index_in_epoch = 0\n","        self.epochs_completed = 0\n","        self.ntrain = self.train_X.size()[0]\n","        self.backup_X = _train_X\n","        self.backup_Y = _train_Y\n","\n","        if generalized:\n","            self.fit_gzsl()\n","        else:\n","            self.fit_zsl()\n","\n","    def pairwise_distances(self, x, y=None):\n","        '''\n","        Input: x is a Nxd matrix\n","               y is an optional Mxd matirx\n","        Output: dist is a NxM matrix where dist[i,j] is the square norm between x[i,:] and y[j,:]\n","                if y is not given then use 'y=x'.\n","        i.e. dist[i,j] = ||x[i,:]-y[j,:]||^2\n","        '''\n","        x_norm = (x ** 2).sum(1).view(-1, 1)\n","        if y is not None:\n","            y_t = torch.transpose(y, 0, 1)\n","            y_norm = (y ** 2).sum(1).view(1, -1)\n","        else:\n","            y_t = torch.transpose(x, 0, 1)\n","            y_norm = x_norm.view(1, -1)\n","\n","        dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)\n","        # Ensure diagonal is zero if x=y\n","        if y is None:\n","            dist = dist - torch.diag(dist.diag)\n","        return torch.clamp(dist, 0.0, np.inf)\n","\n","    def fit_zsl(self):\n","        first_acc = 0\n","        for epoch in range(self.nepoch):\n","            for i in range(0, self.ntrain, self.batch_size):\n","                self.model.zero_grad()\n","                batch_input, batch_label = self.next_batch(self.batch_size)\n","                self.input.copy_(batch_input)\n","                self.label.copy_(batch_label)\n","                inputv = Variable(self.input)  # fake_feature\n","                labelv = Variable(self.label)  # fake_labels\n","                output = self.model(inputv)\n","                loss = self.criterion(output, labelv)  # 使用fake_unseen_feature和labels来训练分类器\n","                loss.backward()\n","                self.optimizer.step()\n","            # using real testing data (of unseen classes) to test classifier2\n","\n","            # testing only hit@1\n","            overall_acc, acc_of_all = self.val_zsl(self.test_unseen_feature, self.test_unseen_label,\n","                                                                 self.unseenclasses)\n","\n","\n","            #  get the highest evaluation result\n","            if overall_acc > first_acc:\n","                first_acc = overall_acc\n","\n","\n","        print('First Acc: {:.2f}%'.format(first_acc * 100))\n","\n","    # for gzsl\n","    def fit_gzsl(self):\n","        # 3个length\n","        # test_seen_length = self.test_seen_feature.shape[0]  # 1764\n","        # test_unseen_length = self.test_unseen_feature.shape[0]  # 2967\n","        # all_length = test_seen_length + test_unseen_length\n","        all_test_feature = torch.cat((self.test_seen_feature, self.test_unseen_feature), 0)\n","        all_test_label = torch.cat((self.test_seen_label, self.test_unseen_label), 0)\n","        all_classes = torch.cat((self.seenclasses, self.unseenclasses), 0)\n","        first_acc = 0\n","        first_all_pred = None\n","        first_all_output = None\n","\n","        best_H = 0\n","        seen_acc = 0\n","        unseen_acc = 0\n","        for epoch in range(self.nepoch):\n","            for i in range(0, self.ntrain, self.batch_size):  # self.ntrain=22057, self.batch_size=300\n","                self.model.zero_grad()\n","                batch_input, batch_label = self.next_batch(self.batch_size)\n","                self.input.copy_(batch_input)\n","                self.label.copy_(batch_label)\n","                inputv = Variable(self.input)\n","                labelv = Variable(self.label)\n","                output = self.model(inputv)\n","                loss = self.criterion(output, labelv)\n","                loss.backward()\n","                self.optimizer.step()\n","\n","            acc_seen, pred_seen = self.val_gzsl(self.test_seen_feature, self.test_seen_label, all_classes,\n","                                                             self.seenclasses, 'seen')\n","            acc_unseen, pred_unseen = self.val_gzsl(self.test_unseen_feature, self.test_unseen_label, all_classes,\n","                                                                   self.unseenclasses, 'unseen')\n","            H = 2 * acc_seen * acc_unseen / (acc_seen + acc_unseen)\n","            if H > best_H:\n","                best_H = H\n","                seen_acc = acc_seen\n","                unseen_acc = acc_unseen\n","        print('First Seen: {:.2f}%, Unseen: {:.2f}%, First H: {:.2f}%'.format(seen_acc * 100,\n","                                                                              unseen_acc * 100,\n","                                                                              best_H * 100))\n","\n","\n","    def val_zsl(self, test_X, test_label, target_classes):\n","        start = 0\n","        ntest = test_X.size()[0]\n","        predicted_label = torch.LongTensor(test_label.size())\n","        for i in range(0, ntest, self.batch_size):\n","            end = min(ntest, start + self.batch_size)\n","            if self.cuda:\n","                output = self.model(Variable(test_X[start:end].cuda(), volatile=True))\n","            else:\n","                output = self.model(Variable(test_X[start:end], volatile=True))\n","\n","            _, predicted_label[start:end] = torch.max(output.data, 1)\n","            start = end\n","        overall_acc = self.compute_acc_avg_per_class(map_label(test_label, target_classes), predicted_label,\n","                                                     target_classes.size(0))\n","        acc_of_all = self.compute_each_class_acc(map_label(test_label, target_classes), predicted_label,\n","                                                 target_classes.size(0))\n","        return overall_acc, acc_of_all\n","\n","    def val_zsl_Hit(self, test_X, test_label, target_classes):\n","        start = 0\n","        ntest = test_X.size()[0]\n","        predicted_label = torch.LongTensor(test_label.size())\n","        predicted_labels = torch.LongTensor(test_label.size(0), target_classes.size(0))\n","        # all_output = None\n","        for i in range(0, ntest, self.batch_size):\n","            end = min(ntest, start + self.batch_size)\n","            if self.cuda:\n","                output = self.model(Variable(test_X[start:end].cuda(), volatile=True))\n","            else:\n","                output = self.model(Variable(test_X[start:end], volatile=True))\n","            _, predicted_label[start:end] = torch.max(output.data, 1)\n","            _, predicted_labels[start:end] = output.data.sort(1, descending=True)\n","            start = end\n","        # print(\"pred shape:\", predicted_labels.shape)\n","        overall_acc = self.compute_acc_avg_per_class(map_label(test_label, target_classes), predicted_label,\n","                                                     target_classes.size(0))\n","        overall_acc_Hit = self.compute_acc_avg_per_class_Hit(map_label(test_label, target_classes), predicted_labels,\n","                                                     target_classes.size(0))\n","\n","        return overall_acc, overall_acc_Hit.squeeze()\n","\n","    def val_gzsl(self, test_X, test_label, all_classes, target_classes, cls_type):\n","        start = 0\n","        ntest = test_X.size()[0]\n","        predicted_label = torch.LongTensor(test_label.size())\n","        for i in range(0, ntest, self.batch_size):\n","            end = min(ntest, start + self.batch_size)\n","            if self.cuda:\n","                output = self.model(Variable(test_X[start:end].cuda(), volatile=True))\n","            else:\n","                output = self.model(Variable(test_X[start:end], volatile=True))\n","\n","            _, predicted_label[start:end] = torch.max(output.data, 1)\n","            start = end\n","        overall_acc = self.compute_acc_avg_per_class_gzsl(map_label(test_label, all_classes), predicted_label, target_classes.size(0), cls_type)\n","        return overall_acc, predicted_label\n","\n","    def next_batch(self, batch_size):\n","        start = self.index_in_epoch\n","        # shuffle the data at the first epoch\n","        if self.epochs_completed == 0 and start == 0:\n","            perm = torch.randperm(self.ntrain)\n","            self.train_X = self.train_X[perm]\n","            self.train_Y = self.train_Y[perm]\n","        # the last batch\n","        if start + batch_size > self.ntrain:\n","            self.epochs_completed += 1\n","            rest_num_examples = self.ntrain - start\n","            if rest_num_examples > 0:\n","                X_rest_part = self.train_X[start:self.ntrain]\n","                Y_rest_part = self.train_Y[start:self.ntrain]\n","            # shuffle the data\n","            perm = torch.randperm(self.ntrain)\n","            self.train_X = self.train_X[perm]\n","            self.train_Y = self.train_Y[perm]\n","            # start next epoch\n","            start = 0\n","            self.index_in_epoch = batch_size - rest_num_examples\n","            end = self.index_in_epoch\n","            X_new_part = self.train_X[start:end]\n","            Y_new_part = self.train_Y[start:end]\n","            if rest_num_examples > 0:\n","                return torch.cat((X_rest_part, X_new_part), 0), torch.cat((Y_rest_part, Y_new_part), 0)\n","            else:\n","                return X_new_part, Y_new_part\n","        else:\n","            self.index_in_epoch += batch_size\n","            end = self.index_in_epoch\n","            return self.train_X[start:end], self.train_Y[start:end]\n","\n","    # def compute_per_class_acc_gzsl\n","    def compute_acc_avg_per_class_gzsl(self, test_label, predicted_label, nclass, cls_type):\n","        acc_per_class = 0\n","        if cls_type == 'seen':\n","            n = 0\n","        if cls_type == 'unseen':\n","            n = self.seenclasses.size(0)\n","        # print(\"n: \", n)\n","        for i in range(nclass):\n","            i = i + n\n","            idx = (test_label == i)\n","            if torch.sum(idx).float() == 0:\n","                continue\n","            else:\n","                acc_per_class += torch.sum(test_label[idx] == predicted_label[idx]).float() / torch.sum(idx).float()\n","        acc_per_class /= nclass\n","        return acc_per_class\n","        # compute Macro metric, i.e., average the accuracy of each class\n","\n","\n","    # def compute_per_class_acc\n","    def compute_acc_avg_per_class(self, test_label, predicted_label, nclass):\n","        acc_per_class = torch.FloatTensor(nclass).fill_(0)\n","        for i in range(nclass):\n","            idx = (test_label == i)\n","            if torch.sum(idx).float() != 0:\n","                acc_per_class[i] = torch.sum(test_label[idx] == predicted_label[idx]).float() / torch.sum(idx).float()\n","        return acc_per_class.mean()\n","\n","    # def compute_per_class_acc\n","    def compute_acc_avg_per_class_Hit(self, test_label, predicted_label, nclass):\n","        top = [1, 2, 5]\n","        acc_per_class = torch.FloatTensor(nclass, len(top)).fill_(0)\n","        for i in range(nclass):\n","            idxs = (test_label == i).nonzero().squeeze()\n","            if torch.sum(idxs).float() != 0:\n","                hits = torch.FloatTensor(top).fill_(0)\n","                for idx in idxs:\n","                    for j in range(len(top)):\n","                        current_top = top[j]\n","                        for sort_id in range(current_top):\n","                            if test_label[idx] == predicted_label[idx][sort_id]:\n","                                hits[j] = hits[j] + 1\n","                                break\n","                # print(\"sum:\", torch.sum(idx))\n","                acc_per_class[i] = hits/idxs.size(0)\n","\n","\n","        return acc_per_class.mean(dim=0, keepdim=True)\n","\n","    # get the accuracy of each class\n","    # def compute_every_class_acc\n","    def compute_each_class_acc(self, test_label, predicted_label, nclass):\n","        acc_per_class = torch.FloatTensor(nclass).fill_(0)\n","        for i in range(nclass):\n","            idx = (test_label == i)\n","            if torch.sum(idx).float() != 0:\n","                acc_per_class[i] = torch.sum(test_label[idx] == predicted_label[idx]).float() / torch.sum(idx).float()\n","        return acc_per_class\n","\n","\n","class LINEAR_LOGSOFTMAX(nn.Module):\n","    def __init__(self, input_dim, nclass):\n","        super(LINEAR_LOGSOFTMAX, self).__init__()\n","        self.fc = nn.Linear(input_dim, nclass)\n","        self.logic = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, x):\n","        o = self.logic(self.fc(x))\n","        return o\n"]},{"cell_type":"markdown","source":["**5. Classifier Pre-training, for supervised classification loss in training stage**"],"metadata":{"id":"N6RWVZRGnXfJ"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"iplp4mTR7Ywk","executionInfo":{"status":"ok","timestamp":1666189377630,"user_tz":-480,"elapsed":3,"user":{"displayName":"黄雨峰","userId":"10355830370640481145"}}},"outputs":[],"source":["class pretrain_CLASSIFIER:\n","    # train_Y is interger\n","    def __init__(self, _train_X, _train_Y, _nclass, _input_dim, _cuda, _lr=0.001, _beta1=0.5, _nepoch=20,\n","                 _batch_size=100):\n","\n","        self.train_X = _train_X\n","        self.train_Y = _train_Y\n","        self.batch_size = _batch_size\n","        self.nepoch = _nepoch\n","        self.nclass = _nclass\n","        self.input_dim = _input_dim\n","        self.cuda = _cuda\n","        self.model = LINEAR_LOGSOFTMAX(self.input_dim, self.nclass)\n","        self.model.apply(weights_init)\n","        self.criterion = nn.NLLLoss()\n","\n","        self.input = torch.FloatTensor(_batch_size, self.input_dim)\n","        self.label = torch.LongTensor(_batch_size)\n","\n","        self.lr = _lr\n","        self.beta1 = _beta1\n","        # setup optimizer\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=_lr, betas=(_beta1, 0.999))\n","\n","        if self.cuda:\n","            self.model.cuda()\n","            self.criterion.cuda()\n","            self.input = self.input.cuda()\n","            self.label = self.label.cuda()\n","\n","        self.index_in_epoch = 0\n","        self.epochs_completed = 0\n","        self.ntrain = self.train_X.size()[0]\n","\n","        self.fit()\n","\n","\n","    def fit(self):\n","        for epoch in range(self.nepoch):\n","            for i in range(0, self.ntrain, self.batch_size):\n","                self.model.zero_grad()\n","                batch_input, batch_label = self.next_batch(self.batch_size)\n","                self.input.copy_(batch_input)\n","                self.label.copy_(batch_label)\n","\n","                inputv = Variable(self.input)\n","                labelv = Variable(self.label)\n","                output = self.model(inputv)\n","                loss = self.criterion(output, labelv)\n","                loss.backward()\n","                self.optimizer.step()\n","\n","    def next_batch(self, batch_size):\n","        start = self.index_in_epoch\n","        # shuffle the data at the first epoch\n","        if self.epochs_completed == 0 and start == 0:\n","            perm = torch.randperm(self.ntrain)\n","            self.train_X = self.train_X[perm]\n","            self.train_Y = self.train_Y[perm]\n","        # the last batch\n","        if start + batch_size > self.ntrain:\n","            self.epochs_completed += 1\n","            rest_num_examples = self.ntrain - start\n","            if rest_num_examples > 0:\n","                X_rest_part = self.train_X[start:self.ntrain]\n","                Y_rest_part = self.train_Y[start:self.ntrain]\n","            # shuffle the data\n","            perm = torch.randperm(self.ntrain)\n","            self.train_X = self.train_X[perm]\n","            self.train_Y = self.train_Y[perm]\n","            # start next epoch\n","            start = 0\n","            self.index_in_epoch = batch_size - rest_num_examples\n","            end = self.index_in_epoch\n","            X_new_part = self.train_X[start:end]\n","            Y_new_part = self.train_Y[start:end]\n","            if rest_num_examples > 0:\n","                return torch.cat((X_rest_part, X_new_part), 0), torch.cat((Y_rest_part, Y_new_part), 0)\n","            else:\n","                return X_new_part, Y_new_part\n","        else:\n","            self.index_in_epoch += batch_size\n","            end = self.index_in_epoch\n","            # from index start to index end-1\n","            return self.train_X[start:end], self.train_Y[start:end]\n","\n","    # test_label is integer\n","    def val(self, test_X, test_label, target_classes):\n","        start = 0\n","        ntest = test_X.size()[0]\n","        predicted_label = torch.LongTensor(test_label.size())\n","        for i in range(0, ntest, self.batch_size):\n","            end = min(ntest, start + self.batch_size)\n","            if self.cuda:\n","                output = self.model(Variable(test_X[start:end].cuda(), volatile=True))\n","            else:\n","                output = self.model(Variable(test_X[start:end], volatile=True))\n","            _, predicted_label[start:end] = torch.max(output.data, 1)\n","            start = end\n","\n","        acc = self.compute_per_class_acc(map_label(test_label, target_classes), predicted_label,\n","                                         target_classes.size(0))\n","        return acc\n","\n","    def compute_per_class_acc(self, test_label, predicted_label, nclass):\n","        acc_per_class = torch.FloatTensor(nclass).fill_(0)\n","        for i in range(nclass):\n","            idx = (test_label == i)\n","            acc_per_class[i] = torch.sum(test_label[idx] == predicted_label[idx]).float() / torch.sum(idx).float()\n","        return acc_per_class.mean()\n","\n","\n","class LINEAR_LOGSOFTMAX(nn.Module):\n","    def __init__(self, input_dim, nclass):\n","        super(LINEAR_LOGSOFTMAX, self).__init__()\n","        self.fc = nn.Linear(input_dim, nclass)\n","        self.logic = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, x):\n","        o = self.logic(self.fc(x))\n","        return o\n"]},{"cell_type":"markdown","source":["**6. OntoZSL model with generator and discriminator**"],"metadata":{"id":"707rJZ3DnwFt"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"gtszXQVl7akk","executionInfo":{"status":"ok","timestamp":1666189391796,"user_tz":-480,"elapsed":4,"user":{"displayName":"黄雨峰","userId":"10355830370640481145"}}},"outputs":[],"source":["def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Linear') != -1:\n","        m.weight.data.normal_(0.0, 0.02)\n","        m.bias.data.fill_(0)\n","    elif classname.find('BatchNorm') != -1:\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)\n","\n","class MLP_CRITIC(nn.Module):\n","    def __init__(self, args):\n","        super(MLP_CRITIC, self).__init__()\n","        self.fc1 = nn.Linear(args.feat_dim + args.sem_dim, args.NDH)\n","        # self.fc2 = nn.Linear(opt.ndh, opt.ndh)\n","        self.fc2 = nn.Linear(args.NDH, 1)\n","        self.lrelu = nn.LeakyReLU(0.2, True)\n","\n","        self.apply(weights_init)\n","\n","    def forward(self, x, sem):\n","        h = torch.cat((x, sem), 1)\n","        h = self.lrelu(self.fc1(h))\n","        h = self.fc2(h)\n","        return h\n","\n","class MLP_G(nn.Module):\n","    def __init__(self, args):\n","        super(MLP_G, self).__init__()\n","        self.fc1 = nn.Linear(args.sem_dim + args.noise_size, args.NGH)\n","        self.fc2 = nn.Linear(args.NGH, args.feat_dim)\n","        self.lrelu = nn.LeakyReLU(0.2, True)\n","        self.relu = nn.ReLU(True)\n","\n","        self.apply(weights_init)\n","\n","    def forward(self, noise, sem):\n","        h = torch.cat((noise, sem), 1)\n","        h = self.lrelu(self.fc1(h))\n","        h = self.relu(self.fc2(h))\n","        return h"]},{"cell_type":"markdown","source":["**7. Model training**"],"metadata":{"id":"_JImNZXyn_qN"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"oUIH8RXx7fwT","executionInfo":{"status":"ok","timestamp":1666189400221,"user_tz":-480,"elapsed":661,"user":{"displayName":"黄雨峰","userId":"10355830370640481145"}}},"outputs":[],"source":["class Runner:\n","    def __init__(self, args):\n","        self.args = args\n","\n","        print('============ Params ============')\n","        print('\\n'.join('%s: %s' % (k, str(v)) for k, v\n","                        in sorted(dict(vars(self.args)).items())))\n","        print('============================================')\n","\n","\n","        # load data\n","        self.data = DATA_LOADER(self.args)\n","        self.feat_dim = self.data.feat_dim\n","        self.sem_dim = self.data.sem_dim\n","        self.semantic = self.data.semantic\n","        print(\"Training samples: \", self.data.ntrain)  # number of training samples\n","\n","        args.feat_dim = self.feat_dim\n","        args.sem_dim = self.sem_dim\n","\n","        # initialize generator and discriminator\n","        self.netG = MLP_G(args)\n","        self.netD = MLP_CRITIC(args)\n","        # setup optimizer\n","        self.optimizerD = optim.Adam(self.netD.parameters(), lr=args.lr, betas=(args.beta, 0.999))\n","        self.optimizerG = optim.Adam(self.netG.parameters(), lr=args.lr, betas=(args.beta, 0.999))\n","        # classification loss\n","        self.cls_criterion = nn.NLLLoss()  # cross entropy loss\n","\n","        self.input_fea = torch.FloatTensor(args.batch_size, self.feat_dim)  # (64, 2048)\n","        self.input_sem = torch.FloatTensor(args.batch_size, self.data.sem_dim)  # (64, 500)\n","        self.noise = torch.FloatTensor(args.batch_size, args.noise_size)  # (64, 500)\n","        self.input_label = torch.LongTensor(args.batch_size)\n","\n","        if self.args.cuda:\n","            self.netD.cuda()\n","            self.netG.cuda()\n","            self.input_fea = self.input_fea.cuda()\n","            self.noise, self.input_sem = self.noise.cuda(), self.input_sem.cuda()\n","            self.cls_criterion.cuda()\n","            self.input_label = self.input_label.cuda()\n","\n","        # train a classifier on seen classes, obtain \\theta of Equation (4)\n","        self.pretrain_cls = pretrain_CLASSIFIER(self.data.train_seen_feature,\n","                                                      map_label(self.data.train_seen_label, self.data.seenclasses),\n","                                                      self.data.seenclasses.size(0), self.feat_dim, args.cuda, 0.001, 0.5,\n","                                                      100, 2 * args.batch_size)\n","\n","        # freeze the classifier during the optimization\n","        for p in self.pretrain_cls.model.parameters():  # set requires_grad to False\n","            p.requires_grad = False\n","\n","\n","    def sample(self):\n","        batch_feature, batch_label, batch_sem = self.data.next_batch(args.batch_size)\n","        self.input_fea.copy_(batch_feature)\n","        self.input_sem.copy_(batch_sem)\n","        self.input_label.copy_(map_label(batch_label, self.data.seenclasses))\n","\n","\n","    def generate_syn_feature(self, num):\n","        classes = self.data.unseenclasses\n","        nclass = classes.size(0)\n","        syn_feature = torch.FloatTensor(nclass * num, self.feat_dim)\n","        syn_label = torch.LongTensor(nclass * num)\n","        syn_sem = torch.FloatTensor(num, self.sem_dim)\n","        syn_noise = torch.FloatTensor(num, args.noise_size)\n","        if self.args.cuda:\n","            syn_sem = syn_sem.cuda()\n","            syn_noise = syn_noise.cuda()\n","        for i in range(nclass):\n","            iclass = classes[i]\n","            iclass_sem = self.semantic[iclass]\n","            syn_sem.copy_(iclass_sem.repeat(num, 1))\n","            syn_noise.normal_(0, 1)\n","            output = self.netG(Variable(syn_noise, volatile=True), Variable(syn_sem, volatile=True))\n","            syn_feature.narrow(0, i * num, num).copy_(output.data.cpu())\n","            syn_label.narrow(0, i * num, num).fill_(iclass)\n","        return syn_feature, syn_label\n","\n","\n","\n","    # the last item of equation (2)\n","    def calc_gradient_penalty(self, real_data, fake_data, input_sem):\n","        # print real_data.size()\n","        alpha = torch.rand(args.batch_size, 1)\n","        alpha = alpha.expand(real_data.size())\n","        if args.cuda:\n","            alpha = alpha.cuda()\n","\n","        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","\n","        if args.cuda:\n","            interpolates = interpolates.cuda()\n","\n","        interpolates = Variable(interpolates, requires_grad=True)\n","\n","        disc_interpolates = self.netD(interpolates, Variable(input_sem))\n","\n","        ones = torch.ones(disc_interpolates.size())\n","        if args.cuda:\n","            ones = ones.cuda()\n","\n","        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n","                                  grad_outputs=ones,\n","                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\n","        # args.GP_Weight = 10\n","        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * args.GP_weight\n","        return gradient_penalty\n","\n","\n","\n","\n","\n","    def train(self):\n","        one = torch.tensor(1, dtype=torch.float)\n","        mone = one * -1\n","        if self.args.cuda:\n","            one = one.cuda()\n","            mone = mone.cuda()\n","\n","        for epoch in range(args.epoch):\n","            for i in range(0, self.data.ntrain, args.batch_size):\n","                # print(\"batch...\", i)\n","                # iteratively train the generator and discriminator\n","                for p in self.netD.parameters():\n","                    p.requires_grad = True\n","\n","                # DISCRIMINATOR\n","                # args.critic_iter = 5, following WGAN-GP\n","                for iter_d in range(args.critic_iter):\n","                    self.sample()  # sample by batch\n","                    self.netD.zero_grad()\n","                    # torch.gt: compare the 'input_res[1]' and '0' element by element\n","                    input_feav = Variable(self.input_fea)\n","                    input_semv = Variable(self.input_sem)\n","\n","                    # loss of real data\n","                    criticD_real = self.netD(input_feav, input_semv)\n","                    criticD_real = criticD_real.mean()\n","                    criticD_real.backward(mone)\n","                    # loss of generated data\n","                    self.noise.normal_(0, 1)\n","                    noisev = Variable(self.noise)\n","                    fake = self.netG(noisev, input_semv)   # generate samples\n","                    # detach(): return a new variable, do not compute gradient for it\n","                    criticD_fake = self.netD(fake.detach(), input_semv)\n","                    criticD_fake = criticD_fake.mean()\n","                    criticD_fake.backward(one)\n","\n","                    # loss with Lipschitz constraint\n","                    gradient_penalty = self.calc_gradient_penalty(self.input_fea, fake.data, self.input_sem)\n","                    gradient_penalty.backward()\n","\n","                    # Wasserstein_D = criticD_real - criticD_fake\n","                    # Final Loss of Discriminator\n","                    D_cost = criticD_fake - criticD_real + gradient_penalty\n","                    self.optimizerD.step()\n","\n","                for p in self.netD.parameters():  # reset requires_grad\n","                    p.requires_grad = False  # avoid computation\n","                # GENERATOR\n","                self.netG.zero_grad()\n","                input_semv = Variable(self.input_sem)\n","                self.noise.normal_(0, 1)\n","                noisev = Variable(self.noise)\n","                fake = self.netG(noisev, input_semv)\n","                criticG_fake = self.netD(fake, input_semv)\n","                criticG_fake = criticG_fake.mean()\n","                G_cost = -criticG_fake\n","                # classification loss\n","                c_errG = self.cls_criterion(self.pretrain_cls.model(fake), Variable(self.input_label))\n","\n","                errG = G_cost + args.cls_weight * c_errG\n","\n","                errG.backward()\n","                self.optimizerG.step()\n","\n","            print('EP[%d/%d]******************************************************' % (epoch, args.epoch))\n","\n","            # evaluate the model, set G to evaluation mode\n","            self.netG.eval()\n","            # train_X: input features (of unseen or seen) for training classifier2 in testing stage\n","            # train_Y: training labels\n","            # Generalized zero-shot learning\n","            if args.gzsl:\n","                syn_feature, syn_label = self.generate_syn_feature(args.syn_num)\n","                if args.dataset == 'AwA2':\n","                    train_X = torch.cat((self.data.train_seen_feature, syn_feature), 0)\n","                    train_Y = torch.cat((self.data.train_seen_label, syn_label), 0)\n","                    classes = torch.cat((self.data.seenclasses, self.data.unseenclasses), 0)\n","                    nclass = classes.size(0)\n","                    CLASSIFIER(args, train_X, map_label(train_Y, classes), self.data, nclass, args.cuda,\n","                                                    args.cls_lr, 0.5, 50, 2 * args.syn_num, True)\n","                else:\n","                    train_X = torch.cat((self.data.train_seen_feature_sub, syn_feature), 0)\n","                    train_Y = torch.cat((self.data.train_seen_label_sub, syn_label), 0)\n","                    classes = torch.cat((self.data.seenclasses, self.data.unseenclasses), 0)\n","                    nclass = classes.size(0)\n","                    CLASSIFIER(args, train_X, map_label(train_Y, classes), self.data, nclass, args.cuda,\n","                                                        args.cls_lr, 0.5, 50, 2 * args.batch_size, True)\n","\n","            # Zero-shot learning\n","            else:\n","                # synthesize samples of unseen classes, for training classifier2 in testing stage\n","                syn_feature, syn_label = self.generate_syn_feature(args.syn_num)\n","                CLASSIFIER(args, syn_feature, map_label(syn_label, self.data.unseenclasses), self.data,\n","                                                 self.data.unseenclasses.size(0), args.cuda, args.cls_lr, 0.5, 50, 10*args.syn_num, False, args.ratio, epoch)\n","\n","            self.netG.train()\n","            # sys.stdout.flush()"]},{"cell_type":"markdown","source":["**8. Parameter Settings and Run Model**"],"metadata":{"id":"YtuTlu0ZoE_y"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"qJ-t-UIo7gUW","outputId":"6659a636-f5c2-40d3-fa50-a45ce97b62e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Seed:  9416\n","using gpu 0\n","2022-10-12 16:16:23\n","Begin run!!!\n","============ Params ============\n","GP_weight: 10\n","NDH: 4096\n","NGH: 4096\n","batch_size: 4096\n","beta: 0.5\n","cls_lr: 0.001\n","cls_weight: 0.01\n","critic_iter: 5\n","cuda: True\n","data_dir: /content/drive/MyDrive/ISWC_demo/ZS_IMGC/data\n","dataset: ImNet_A\n","epoch: 100\n","gpu: 0\n","gzsl: False\n","lr: 0.0001\n","manual_seed: 9416\n","noise_size: 100\n","ratio: 0.1\n","semantic_type: kge\n","syn_num: 300\n","============================================\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:94: DeprecationWarning: 'U' mode is deprecated\n"]},{"name":"stdout","output_type":"stream","text":["WARNING: invalid semantic embeddings type\n","Training samples:  35150\n","EP[0/100]******************************************************\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:76: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:161: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"]},{"name":"stdout","output_type":"stream","text":["First Acc: 3.87%\n","EP[1/100]******************************************************\n","First Acc: 6.21%\n","EP[2/100]******************************************************\n","First Acc: 12.57%\n","EP[3/100]******************************************************\n","First Acc: 15.73%\n","EP[4/100]******************************************************\n","First Acc: 20.73%\n","EP[5/100]******************************************************\n","First Acc: 23.40%\n","EP[6/100]******************************************************\n","First Acc: 28.33%\n","EP[7/100]******************************************************\n","First Acc: 31.13%\n","EP[8/100]******************************************************\n","First Acc: 31.61%\n","EP[9/100]******************************************************\n","First Acc: 31.07%\n","EP[10/100]******************************************************\n","First Acc: 33.16%\n","EP[11/100]******************************************************\n","First Acc: 33.75%\n","EP[12/100]******************************************************\n","First Acc: 34.66%\n","EP[13/100]******************************************************\n","First Acc: 35.24%\n","EP[14/100]******************************************************\n","First Acc: 36.11%\n","EP[15/100]******************************************************\n","First Acc: 37.23%\n","EP[16/100]******************************************************\n","First Acc: 37.98%\n","EP[17/100]******************************************************\n","First Acc: 38.02%\n","EP[18/100]******************************************************\n","First Acc: 37.59%\n","EP[19/100]******************************************************\n","First Acc: 37.18%\n","EP[20/100]******************************************************\n","First Acc: 37.94%\n","EP[21/100]******************************************************\n","First Acc: 37.43%\n","EP[22/100]******************************************************\n","First Acc: 36.41%\n","EP[23/100]******************************************************\n","First Acc: 36.43%\n","EP[24/100]******************************************************\n","First Acc: 35.94%\n","EP[25/100]******************************************************\n","First Acc: 35.84%\n","EP[26/100]******************************************************\n","First Acc: 36.61%\n","EP[27/100]******************************************************\n","First Acc: 35.74%\n","EP[28/100]******************************************************\n","First Acc: 35.42%\n","EP[29/100]******************************************************\n","First Acc: 34.76%\n","EP[30/100]******************************************************\n","First Acc: 34.52%\n","EP[31/100]******************************************************\n","First Acc: 34.78%\n","EP[32/100]******************************************************\n","First Acc: 34.82%\n","EP[33/100]******************************************************\n","First Acc: 35.09%\n","EP[34/100]******************************************************\n","First Acc: 35.37%\n","EP[35/100]******************************************************\n","First Acc: 34.66%\n","EP[36/100]******************************************************\n","First Acc: 35.90%\n","EP[37/100]******************************************************\n","First Acc: 34.29%\n","EP[38/100]******************************************************\n","First Acc: 34.89%\n","EP[39/100]******************************************************\n","First Acc: 34.48%\n","EP[40/100]******************************************************\n","First Acc: 34.29%\n","EP[41/100]******************************************************\n","First Acc: 35.65%\n","EP[42/100]******************************************************\n","First Acc: 34.95%\n","EP[43/100]******************************************************\n","First Acc: 34.47%\n","EP[44/100]******************************************************\n","First Acc: 34.26%\n","EP[45/100]******************************************************\n","First Acc: 34.01%\n","EP[46/100]******************************************************\n","First Acc: 34.65%\n","EP[47/100]******************************************************\n","First Acc: 34.37%\n","EP[48/100]******************************************************\n","First Acc: 34.02%\n","EP[49/100]******************************************************\n","First Acc: 35.34%\n","EP[50/100]******************************************************\n","First Acc: 34.45%\n","EP[51/100]******************************************************\n","First Acc: 34.16%\n","EP[52/100]******************************************************\n","First Acc: 34.64%\n","EP[53/100]******************************************************\n","First Acc: 34.42%\n","EP[54/100]******************************************************\n","First Acc: 34.68%\n","EP[55/100]******************************************************\n","First Acc: 34.58%\n","EP[56/100]******************************************************\n","First Acc: 34.79%\n","EP[57/100]******************************************************\n","First Acc: 34.66%\n","EP[58/100]******************************************************\n","First Acc: 34.39%\n","EP[59/100]******************************************************\n","First Acc: 34.75%\n","EP[60/100]******************************************************\n","First Acc: 34.46%\n","EP[61/100]******************************************************\n","First Acc: 35.20%\n","EP[62/100]******************************************************\n","First Acc: 35.01%\n","EP[63/100]******************************************************\n","First Acc: 35.59%\n","EP[64/100]******************************************************\n","First Acc: 34.25%\n","EP[65/100]******************************************************\n","First Acc: 34.17%\n","EP[66/100]******************************************************\n","First Acc: 34.62%\n","EP[67/100]******************************************************\n","First Acc: 35.21%\n","EP[68/100]******************************************************\n","First Acc: 34.48%\n","EP[69/100]******************************************************\n","First Acc: 34.83%\n","EP[70/100]******************************************************\n","First Acc: 35.52%\n","EP[71/100]******************************************************\n","First Acc: 34.79%\n","EP[72/100]******************************************************\n","First Acc: 34.86%\n","EP[73/100]******************************************************\n","First Acc: 35.09%\n","EP[74/100]******************************************************\n","First Acc: 35.37%\n","EP[75/100]******************************************************\n","First Acc: 35.52%\n","EP[76/100]******************************************************\n","First Acc: 35.16%\n","EP[77/100]******************************************************\n","First Acc: 35.96%\n","EP[78/100]******************************************************\n","First Acc: 34.58%\n","EP[79/100]******************************************************\n","First Acc: 35.74%\n","EP[80/100]******************************************************\n","First Acc: 34.39%\n","EP[81/100]******************************************************\n","First Acc: 35.12%\n","EP[82/100]******************************************************\n","First Acc: 35.66%\n","EP[83/100]******************************************************\n","First Acc: 35.18%\n","EP[84/100]******************************************************\n","First Acc: 35.37%\n","EP[85/100]******************************************************\n","First Acc: 35.04%\n","EP[86/100]******************************************************\n","First Acc: 35.47%\n","EP[87/100]******************************************************\n","First Acc: 35.34%\n","EP[88/100]******************************************************\n","First Acc: 34.79%\n","EP[89/100]******************************************************\n","First Acc: 35.68%\n","EP[90/100]******************************************************\n","First Acc: 35.32%\n","EP[91/100]******************************************************\n","First Acc: 36.26%\n","EP[92/100]******************************************************\n","First Acc: 35.11%\n","EP[93/100]******************************************************\n","First Acc: 35.74%\n","EP[94/100]******************************************************\n","First Acc: 35.28%\n","EP[95/100]******************************************************\n","First Acc: 34.81%\n","EP[96/100]******************************************************\n","First Acc: 35.44%\n","EP[97/100]******************************************************\n","First Acc: 35.21%\n","EP[98/100]******************************************************\n","First Acc: 35.00%\n","EP[99/100]******************************************************\n","First Acc: 35.44%\n","End run!!!\n","2022-10-12 18:06:15\n"]}],"source":["if __name__ == '__main__':\n","\n","    parser = argparse.ArgumentParser()\n","    '''\n","    Data loading\n","    '''\n","\n","    parser.add_argument('--data_dir', default='/content/drive/MyDrive/ISWC_demo/ZS_IMGC/data', help='path to save dataset')\n","    parser.add_argument('--dataset', default='ImNet_A', help='target datasets, options: {AwA2, ImNet_A, ImNet_O}')\n","\n","    parser.add_argument('--semantic_type', default='kge', type=str, help='the type of class embedding to input, options: {att, w2v, w2v-glove, hie, kge (Basic KG), kge_text (Basic KG+literal), kge_facts (Basic KG+CN), kge_logics (Basic KG+logics)}')\n","    parser.add_argument('--noise_size', type=int, default=100, help='size of noise vectors')\n","    '''\n","    Generator and Discriminator Parameter\n","    '''\n","    parser.add_argument('--NGH', default=4096, help='size of the hidden units in generator')\n","    parser.add_argument('--NDH', default=4096, help='size of the hidden units in discriminator')\n","    parser.add_argument('--critic_iter', default=5, help='critic iteration of discriminator, default=5, following WGAN-GP setting')\n","    parser.add_argument('--GP_weight', type=float, default=10, help='gradient penalty regularizer, default=10, the completion of Lipschitz Constraint in WGAN-GP')\n","    parser.add_argument('--cls_weight', default=0.01, help='loss weight for the supervised classification loss')\n","    parser.add_argument('--syn_num', default=300, type=int, help='number of features generating for each unseen class; awa_default = 300')\n","    '''\n","    Training Parameter\n","    '''\n","    parser.add_argument('--gzsl', action='store_true', default=False, help='enable generalized zero-shot learning')\n","    parser.add_argument('--cuda', default=True, help='')\n","    parser.add_argument(\"--gpu\", type=int, default=0, help=\"Which GPU to use?\")\n","    parser.add_argument('--manual_seed', default=9416, type=int, help='random seed')  #\n","    parser.add_argument('--batch_size', default=4096, type=int, help='training batch size')\n","    parser.add_argument('--epoch', default=100, help='training epoch')\n","    parser.add_argument('--lr', default=0.0001, type=float, help='learning rate to train GAN')\n","    parser.add_argument('--cls_lr', default=0.001, help='after generating unseen features, the learning rate for training softmax classifier')\n","    parser.add_argument('--ratio', default=0.1, help='ratio of easy samples')\n","    parser.add_argument('--beta', default=0.5, help='beta for adam, default=0.5')\n","\n","\n","    args = parser.parse_known_args()[0]\n","\n","    if args.manual_seed is None:\n","        args.manual_seed = random.randint(1, 10000)\n","    print(\"Random Seed: \", args.manual_seed)\n","\n","    np.random.seed(args.manual_seed)\n","    random.seed(args.manual_seed)\n","    torch.manual_seed(args.manual_seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.set_device(args.gpu)\n","        print('using gpu {}'.format(args.gpu))\n","        torch.cuda.manual_seed_all(args.manual_seed)\n","        torch.backends.cudnn.deterministic = True\n","\n","\n","\n","\n","    print(GetNowTime())\n","    print('Begin run!!!')\n","\n","    run = Runner(args)\n","    run.train()\n","\n","    print('End run!!!')\n","    print(GetNowTime())"]},{"cell_type":"markdown","source":["**Parameters in other Settings**\n","\n","\n","---\n","\n","*   **run OntoZSL on ImNet-A with Basic KG in the Generalized ZSL setting**\n","\n","============ Params ============\n","GP_weight: 10;\n","NDH: 4096;\n","NGH: 4096;\n","batch_size: 4096;\n","beta: 0.5;\n","cls_lr: 0.001;\n","cls_weight: 0.01;\n","critic_iter: 5;\n","cuda: True;\n","data_dir: /content/drive/MyDrive/ISWC_demo/ZS_IMGC/data;\n","dataset: ImNet_A;\n","epoch: 100;\n","gpu: 0;\n","gzsl: True;\n","lr: 0.0001;\n","manual_seed: 9416;\n","noise_size: 100;\n","ratio: 0.1;\n","semantic_type: kge;\n","syn_num: 300;\n","\n","*   **run OntoZSL on AwA2 with \"Basic KG+literal\" in the Standard ZSL setting**\n","\n","============ Params ============\n","GP_weight: 10;\n","NDH: 4096;\n","NGH: 4096;\n","batch_size: 64;\n","beta: 0.5;\n","cls_lr: 0.001;\n","cls_weight: 0.01;\n","critic_iter: 5;\n","cuda: True;\n","data_dir:/content/drive/MyDrive/ISWC_demo/ZS_IMGC/data;\n","dataset: AwA2;\n","epoch: 100;\n","gpu: 0;\n","gzsl: False;\n","lr: 1e-05;\n","manual_seed: 9182;\n","noise_size: 100;\n","ratio: 0.1;\n","semantic_type: kge_text;\n","syn_num: 300;\n","*   **run OntoZSL on AwA2 with \"Basic KG+literal\" in the Generalized ZSL setting**\n","\n","============ Params ============\n","GP_weight: 10;\n","NDH: 4096;\n","NGH: 4096;\n","batch_size: 64;\n","beta: 0.5;\n","cls_lr: 0.001;\n","cls_weight: 0.01;\n","critic_iter: 5;\n","cuda: True;\n","data_dir: /content/drive/MyDrive/ISWC_demo/ZS_IMGC/data;\n","dataset: AwA2;\n","epoch: 100;\n","gpu: 0;\n","gzsl: True;\n","lr: 1e-05;\n","manual_seed: 9182;\n","noise_size: 100;\n","ratio: 0.1;\n","semantic_type: kge_text;\n","syn_num: 1800;\n","\n","*   **run OntoZSL on ImNet-O with \"att\" in the Standard ZSL setting**\n","\n","============ Params ============\n","GP_weight: 10;\n","NDH: 4096;\n","NGH: 4096;\n","batch_size: 4096;\n","beta: 0.5;\n","cls_lr: 0.001;\n","cls_weight: 0.01;\n","critic_iter: 5;\n","cuda: True;\n","data_dir: /content/drive/MyDrive/ISWC_demo/ZS_IMGC/data;\n","dataset: ImNet_O;\n","epoch: 100;\n","gpu: 0;\n","gzsl: False;\n","lr: 0.0001;\n","manual_seed: 9416;\n","noise_size: 85;\n","ratio: 0.1;\n","semantic_type: att;\n","syn_num: 300;\n","\n","\n","\n","\n","---\n","\n","\n","**The best parameter of noise_size for AwA2**\n","\n","hie, kge, kge_text, kge_facts, kge_logics is set to 100, \n","w2v is set to 500, w2v-glove is set to 300, att is set to 85\n","\n","**The best parameter of noise_size for ImNet_A**\n","\n","hie, kge, kge_text, kge_facts is set to 100, \n","w2v is set to 500, w2v-glove is set to 300, att is set to 85\n","\n","**The best parameter of noise_size for ImNet_O**\n","\n","hie, kge, kge_text, kge_facts is set to 100, \n","w2v is set to 500, w2v-glove is set to 300, att is set to 40"],"metadata":{"id":"MCPi34bssXvj"}}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}