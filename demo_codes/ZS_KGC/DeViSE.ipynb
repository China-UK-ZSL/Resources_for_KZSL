{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Run DeViSE on NELL with RDFS**\n","---\n","You can run other settings by changing the parameters of \"dataset\", \"semantic type\", \"hidden_dims\" and etc.\n","\n","For example, run on NELL with RDFS+literal by changing \"semantic type\" into \"rdfs_text\" and hidden_dims\" into 400.\n","\n","More parameter details are attached in the end."],"metadata":{"id":"ysk50MPLCmrS"}},{"cell_type":"markdown","source":["**1. Bind your Google Drive**"],"metadata":{"id":"Gtth8eK1uDnE"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"I_jXWTPk-hjN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666193852464,"user_tz":-480,"elapsed":3563,"user":{"displayName":"黄雨峰","userId":"10355830370640481145"}},"outputId":"283a65f1-5d53-4cbd-cdcb-3653386f2a5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"markdown","source":["**2. Import Package**"],"metadata":{"id":"IxITPSypuEZs"}},{"cell_type":"code","source":["import random\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.optim as optim\n","from collections import defaultdict\n","from collections import deque\n","from torch.autograd import Variable\n","from tqdm import tqdm\n","from sklearn.metrics.pairwise import cosine_similarity\n","import json\n","import os\n","import json\n","import numpy as np"],"metadata":{"id":"C90Per9kCsKX","executionInfo":{"status":"ok","timestamp":1666193856068,"user_tz":-480,"elapsed":640,"user":{"displayName":"黄雨峰","userId":"10355830370640481145"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["**3. Parameters Setting**"],"metadata":{"id":"g1RzbSlwuHAt"}},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","\n","parser.add_argument('--data_dir', default='/content/drive/MyDrive/ISWC_demo/ZS_KGC/data/')\n","parser.add_argument('--dataset', default='NELL')\n","\n","parser.add_argument(\"--embed_model\", default='TransE', type=str)\n","parser.add_argument(\"--max_neighbor\", default=50, type=int, help='neighbor number of each entity')\n","parser.add_argument(\"--embed_dim\", default=100, type=int, help='dimension of triple embedding')\n","parser.add_argument(\"--ep_dim\", default=200, type=int, help='dimension of entity pair embedding')\n","parser.add_argument(\"--batch_size\", default=64, type=int)\n","parser.add_argument(\"--batch_rela_num\", default=4, type=int)\n","parser.add_argument(\"--train_times\", default=7000, type=int)\n","\n","parser.add_argument(\"--loss_every\", default=50, type=int)\n","parser.add_argument(\"--eval_every\", default=500, type=int)\n","\n","parser.add_argument('--p', default=0.5, help='dropout', type=float)\n","parser.add_argument('--lr', default=1e-3, help='learning rate', type=float)\n","parser.add_argument('--wds', default=1e-5, help='', type=float)\n","parser.add_argument('--manual_seed', default=12345, help='', type=int)\n","parser.add_argument(\"--semantic_type\", default='rdfs', help='the type of relation embedding to input, options: {text, rdfs, rdfs_hie, rdfs_cons, rdfs_text}')\n","parser.add_argument('--hidden_dims', type=int, default=300, help='size of the hidden units in DeViSE')\n","\n","parser.add_argument('--gpu', default=0, help='gpu id', type=int)\n","\n","args = parser.parse_known_args()[0]\n","\n","args.data_path = os.path.join(args.data_dir, args.dataset)\n","args.output_dims = args.ep_dim\n","\n","if args.manual_seed is None:\n","    args.manual_seed = random.randint(1, 10000)\n","print(\"Random Seed: \", args.manual_seed)\n","\n","np.random.seed(args.manual_seed)\n","random.seed(args.manual_seed)\n","torch.manual_seed(args.manual_seed)\n","if torch.cuda.is_available():\n","    torch.cuda.set_device(args.gpu)\n","    print('using gpu {}'.format(args.gpu))\n","    torch.cuda.manual_seed_all(args.manual_seed)\n","    torch.backends.cudnn.deterministic = True\n","else:\n","    print(\"GPU is not available!\")"],"metadata":{"id":"fO5rAs1NC7yu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666193860638,"user_tz":-480,"elapsed":3,"user":{"displayName":"黄雨峰","userId":"10355830370640481145"}},"outputId":"e0390955-8325-4474-af61-c34b3b0eb3f0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Seed:  12345\n","using gpu 0\n"]}]},{"cell_type":"markdown","source":["**4. Loading Data**"],"metadata":{"id":"JcCd1lm0unK2"}},{"cell_type":"code","source":["def load_semantic_embed(data_path, dataset, type):\n","    \"\"\"\n","    Load Semantic Embeddings.\n","\n","    Parameters\n","    ----------\n","    file_name : str\n","        Name of the semantic embedding file.\n","    type: str\n","        Type of semantic embeddings, including\n","\n","    Returns\n","    -------\n","    embeddings : NumPy arrays\n","       the size is\n","    Examples\n","    --------\n","    \"\"\"\n","\n","    file_name = ''\n","    file_path = os.path.join(data_path, 'semantic_embeddings')\n","    if dataset == 'NELL':\n","        if type == 'rdfs':\n","            file_name = os.path.join(file_path, 'rela_matrix_rdfs_55000.npz')\n","        elif type == 'rdfs_hie':\n","            file_name = os.path.join(file_path, 'rela_matrix_rdfs_hie_60000.npz')\n","        elif type == 'rdfs_cons':\n","            file_name = os.path.join(file_path, 'rela_matrix_rdfs_cons_60000.npz')\n","        elif type == 'text':\n","            file_name = os.path.join(file_path, 'rela_matrix_text.npz')\n","        elif type == 'rdfs_text':\n","            file_name = os.path.join(file_path, 'rela_matrix_rdfs_55000_text140.npz')\n","        else:\n","            print(\"WARNING: invalid semantic embeddings type\")\n","    elif dataset == 'Wiki':\n","        if type == 'rdfs':\n","            file_name = os.path.join(file_path, 'rela_matrix_rdfs_65000.npz')\n","        elif type == 'rdfs_hie':\n","            file_name = os.path.join(file_path, 'rela_matrix_rdfs_hie_60000.npz')\n","        elif type == 'rdfs_cons':\n","            file_name = os.path.join(file_path, 'rela_matrix_rdfs_cons_60000.npz')\n","        elif type == 'text':\n","            file_name = os.path.join(file_path, 'rela_matrix_text.npz')\n","        elif type == 'rdfs_text':\n","            file_name = os.path.join(file_path, 'rela_matrix_rdfs_65000_text140.npz')\n","        else:\n","            print(\"WARNING: invalid semantic embeddings type\")\n","\n","\n","\n","\n","\n","    if file_name:\n","        rela_embeddings = np.load(file_name)['relaM'].astype('float32')\n","\n","\n","    else:\n","        print('WARNING: invalid semantic embeddings file path')\n","    return rela_embeddings\n","\n","def random_pick(some_list, probabilities):\n","    x = random.uniform(0,1)\n","    cumulative_probability = 0.0\n","    for item, item_probability in zip(some_list, probabilities):\n","        cumulative_probability += item_probability\n","        if x < cumulative_probability:break\n","    return item\n","\n","def load_train_data(args, train_tasks, symbol2id, ent2id, e1rel_e2, rel2id, rela2label, rela_matrix):\n","    print('##LOADING CANDIDATES')\n","    rel2candidates = json.load(open(os.path.join(args.data_path, 'rel2candidates_all.json')))\n","    task_pool = sorted(train_tasks.keys())  # ensure the readout is the same\n","\n","    # print(task_pool)\n","\n","    while True:\n","        rel_batch, rel_neg_batch, query_pairs, query_left, query_right, false_pairs, false_left, false_right, labels = [], [], [], [], [], [], [], [], []\n","        random.shuffle(task_pool)\n","        if len(rel2candidates[task_pool[0]]) <= 20:\n","            continue\n","        if len(rel2candidates[task_pool[1]]) <= 20:\n","            continue\n","        for query in task_pool[:args.batch_rela_num]:\n","            # print(query)\n","            relation_id = rel2id[query]\n","            candidates = rel2candidates[query]\n","\n","            if args.dataset == 'Wiki':\n","                if len(candidates) <= 20:\n","                    # print 'not enough candidates'\n","                    continue\n","\n","            train_and_test = train_tasks[query]\n","\n","            random.shuffle(train_and_test)\n","\n","            all_test_triples = train_and_test\n","\n","            if len(all_test_triples) == 0:\n","                continue\n","\n","\n","            if len(all_test_triples) < args.batch_size:\n","                query_triples = [random.choice(all_test_triples) for _ in range(args.batch_size)]\n","            else:\n","                query_triples = random.sample(all_test_triples, args.batch_size)\n","\n","            query_pairs += [[symbol2id[triple[0]], symbol2id[triple[2]]] for triple in query_triples]\n","\n","            query_left += [ent2id[triple[0]] for triple in query_triples]\n","            query_right += [ent2id[triple[2]] for triple in query_triples]\n","\n","            label = rela2label[query]\n","\n","            # generate negative samples\n","            false_pairs_ = []\n","            false_left_ = []\n","            false_right_ = []\n","            for triple in query_triples:\n","                e_h = triple[0]\n","                rel = triple[1]\n","                e_t = triple[2]\n","                while True:\n","                    noise = random.choice(candidates)\n","                    if noise in ent2id.keys(): # ent2id.has_key(noise):\n","                        if (noise not in e1rel_e2[e_h+rel]) and noise != e_t:\n","                            break\n","                false_pairs_.append([symbol2id[e_h], symbol2id[noise]])\n","                false_left_.append(ent2id[e_h])\n","                false_right_.append(ent2id[noise])\n","\n","            false_pairs += false_pairs_\n","            false_left += false_left_\n","            false_right += false_right_\n","\n","            rel_batch += [rel2id[query] for _ in range(args.batch_size)]\n","\n","            neg_rel_batch = list()\n","            for _ in range(args.batch_size):\n","                while True:\n","                    neg_query = random.choice(task_pool)\n","                    if neg_query != query:\n","                        break\n","\n","                neg_rel_batch.append(rel2id[neg_query])\n","\n","            rel_neg_batch += neg_rel_batch\n","\n","\n","            labels += [rela2label[query]] * args.batch_size\n","\n","        yield rela_matrix[rel_batch], rela_matrix[rel_neg_batch], query_pairs, query_left, query_right, false_pairs, false_left, false_right, labels\n"],"metadata":{"id":"xzu4VEkwDiB_","executionInfo":{"status":"ok","timestamp":1666193865362,"user_tz":-480,"elapsed":3,"user":{"displayName":"黄雨峰","userId":"10355830370640481145"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["**5. Models (DeViSE and Feature Encoder)**"],"metadata":{"id":"exahvvzyukqo"}},{"cell_type":"code","source":["def weights_init(m):\n","    classname = m.__class__.__name__\n","    if 'Linear' in classname:\n","        init.xavier_normal_(m.weight.data)\n","        init.constant_(m.bias, 0.0)\n","\n","\n","class LayerNormalization(nn.Module):\n","    ''' Layer normalization module '''\n","\n","    def __init__(self, d_hid, eps=1e-3):\n","        super(LayerNormalization, self).__init__()\n","\n","        self.eps = eps\n","        self.a_2 = nn.Parameter(torch.ones(d_hid), requires_grad=True)\n","        self.b_2 = nn.Parameter(torch.zeros(d_hid), requires_grad=True)\n","\n","    def forward(self, z):\n","        if z.size(1) == 1:\n","            return z\n","\n","        mu = torch.mean(z, keepdim=True, dim=-1)\n","        sigma = torch.std(z, keepdim=True, dim=-1)\n","        ln_out = (z - mu.expand_as(z)) / (sigma.expand_as(z) + self.eps)\n","        ln_out = ln_out * self.a_2.expand_as(ln_out) + self.b_2.expand_as(ln_out)\n","\n","        return ln_out\n","\n","class SupportEncoder(nn.Module):\n","    \"\"\"docstring for SupportEncoder\"\"\"\n","    def __init__(self, d_model, d_inner, dropout=0.1):\n","        super(SupportEncoder, self).__init__()\n","        self.proj1 = nn.Linear(d_model, d_inner)\n","        self.proj2 = nn.Linear(d_inner, d_model)\n","        self.layer_norm = LayerNormalization(d_model)\n","\n","        init.xavier_normal_(self.proj1.weight)\n","        init.xavier_normal_(self.proj2.weight)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        residual = x\n","        output = self.relu(self.proj1(x))\n","        output = self.dropout(self.proj2(output))\n","\n","\n","        return self.layer_norm(output + residual)\n","\n","class Extractor(nn.Module):\n","    \"\"\"\n","    Matching metric based on KB Embeddings\n","    \"\"\"\n","\n","    def __init__(self, embed_dim, num_symbols, embed=None):\n","        super(Extractor, self).__init__()\n","        self.embed_dim = int(embed_dim)\n","        self.pad_idx = num_symbols\n","        self.symbol_emb = nn.Embedding(num_symbols + 1, embed_dim, padding_idx=num_symbols)\n","        self.num_symbols = num_symbols\n","\n","        self.gcn_w = nn.Linear(self.embed_dim, int(self.embed_dim / 2))\n","        self.gcn_b = nn.Parameter(torch.FloatTensor(self.embed_dim))\n","\n","        self.fc1 = nn.Linear(self.embed_dim, int(self.embed_dim / 2))\n","        self.fc2 = nn.Linear(self.embed_dim, int(self.embed_dim / 2))\n","\n","        self.dropout = nn.Dropout(0.2)\n","        self.dropout_e = nn.Dropout(0.2)\n","\n","        self.symbol_emb.weight.data.copy_(torch.from_numpy(embed))\n","\n","        self.symbol_emb.weight.requires_grad = False\n","\n","        d_model = self.embed_dim * 2\n","        self.support_encoder = SupportEncoder(d_model, 2 * d_model, dropout=0.2)\n","        # self.query_encoder = QueryEncoder(d_model, process_steps)\n","\n","    def neighbor_encoder(self, connections, num_neighbors):\n","        '''\n","        connections: (batch, 200, 2)\n","        num_neighbors: (batch,)\n","        '''\n","        num_neighbors = num_neighbors.unsqueeze(1)\n","        entities = connections[:, :, 1].squeeze(-1)\n","        ent_embeds = self.dropout(self.symbol_emb(entities))  # (batch, 50, embed_dim)\n","        concat_embeds = ent_embeds\n","\n","        out = self.gcn_w(concat_embeds)\n","        out = torch.sum(out, dim=1)  # (batch, embed_dim)\n","        out = out / num_neighbors\n","        return out.tanh()\n","\n","    def entity_encoder(self, entity1, entity2):\n","        entity1 = self.dropout_e(entity1)\n","        entity2 = self.dropout_e(entity2)\n","        entity1 = self.fc1(entity1)\n","        entity2 = self.fc2(entity2)\n","        entity = torch.cat((entity1, entity2), dim=-1)\n","        return entity.tanh()  # (batch, embed_dim)\n","\n","    def forward(self, query, support, query_meta=None, support_meta=None):\n","        '''\n","        query: (batch_size, 2)\n","        support: (few, 2)\n","        return: (batch_size, )\n","        '''\n","        query_left_connections, query_left_degrees, query_right_connections, query_right_degrees = query_meta\n","        support_left_connections, support_left_degrees, support_right_connections, support_right_degrees = support_meta\n","\n","        query_e1 = self.symbol_emb(query[:, 0])  # (batch, embed_dim)\n","        query_e2 = self.symbol_emb(query[:, 1])  # (batch, embed_dim)\n","        query_e = self.entity_encoder(query_e1, query_e2)\n","\n","        support_e1 = self.symbol_emb(support[:, 0])  # (batch, embed_dim)\n","        support_e2 = self.symbol_emb(support[:, 1])  # (batch, embed_dim)\n","        support_e = self.entity_encoder(support_e1, support_e2)\n","\n","        query_left = self.neighbor_encoder(query_left_connections, query_left_degrees)\n","        query_right = self.neighbor_encoder(query_right_connections, query_right_degrees)\n","\n","        support_left = self.neighbor_encoder(support_left_connections, support_left_degrees)\n","        support_right = self.neighbor_encoder(support_right_connections, support_right_degrees)\n","\n","        query_neighbor = torch.cat((query_left, query_e, query_right), dim=-1)  # tanh\n","        support_neighbor = torch.cat((support_left, support_e, support_right), dim=-1)  # tanh\n","\n","        support = support_neighbor\n","        query = query_neighbor\n","\n","        support_g = self.support_encoder(support)  # 1 * 100\n","        query_g = self.support_encoder(query)\n","\n","\n","\n","        support_g = torch.mean(support_g, dim=0, keepdim=True)\n","\n","        # cosine similarity\n","        matching_scores = torch.matmul(query_g, support_g.t()).squeeze()\n","\n","        return query_g, matching_scores\n","\n","\n","\n","class DeViSE(nn.Module):\n","    def __init__(self, input_dims, hidden_dims, output_dims, p):\n","        super(DeViSE, self).__init__()\n","        self.model = nn.Sequential(nn.BatchNorm1d(input_dims),\n","                         nn.Dropout(p),\n","                         nn.Linear(in_features=input_dims, out_features=hidden_dims, bias=True),\n","                         nn.ReLU(),\n","                         nn.BatchNorm1d(hidden_dims),\n","                         nn.Dropout(p),\n","                         nn.Linear(in_features=hidden_dims, out_features=output_dims, bias=True))\n","    def forward(self, x):\n","        x = self.model(x)\n","        return x"],"metadata":{"id":"sWFsTrsgDvIe","executionInfo":{"status":"ok","timestamp":1666193875676,"user_tz":-480,"elapsed":561,"user":{"displayName":"黄雨峰","userId":"10355830370640481145"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["**6. Model Training and Testing**"],"metadata":{"id":"nTRy__pku95N"}},{"cell_type":"code","source":["class Runner:\n","    def __init__(self, args):\n","        self.args = args\n","\n","        print('############## LOADING .... ###################')\n","\n","        self.train_tasks = json.load(open(os.path.join(args.data_path, 'datasplit', 'train_tasks.json')))\n","        self.rel2id = json.load(open(os.path.join(args.data_path, 'relation2ids')))\n","\n","        self.rela_matrix = load_semantic_embed(args.data_path, args.dataset, args.semantic_type)\n","        self.input_dims = self.rela_matrix.shape[1]\n","\n","        print('##LOADING ENTITY##')\n","        self.ent2id = json.load(open(os.path.join(args.data_path, 'entity2id')))\n","        num_ents = len(self.ent2id.keys())\n","\n","        print('##LOADING CANDIDATES ENTITIES##')\n","        self.rel2candidates = json.load(open(os.path.join(args.data_path, 'rel2candidates_all.json')))\n","\n","        # load answer dict\n","        self.e1rel_e2 = defaultdict(list)\n","        self.e1rel_e2 = json.load(open(os.path.join(args.data_path, 'e1rel_e2_all.json')))\n","\n","\n","        self.rela2label = dict()\n","        rela_sorted = sorted(list(self.train_tasks.keys()))\n","        for i, rela in enumerate(rela_sorted):\n","            self.rela2label[rela] = int(i)\n","\n","        print('##LOADING SYMBOL ID AND SYMBOL EMBEDDING')\n","        self.symbol2id, self.symbol2vec = self.read_embed()\n","\n","        num_symbols = len(self.symbol2id.keys()) - 1  #\n","        print(\"num symbols:\", num_symbols)\n","        pad_id = num_symbols\n","\n","        # Pretraining step to obtain reasonable real data embeddings, load already pre-trained\n","        print('Load Pretrained Feature Encoder!')\n","        feature_encoder = Extractor(args.embed_dim, num_symbols, embed=self.symbol2vec).cuda()\n","        feature_encoder.apply(weights_init)\n","        model_path = os.path.join(args.data_path, 'FE_models_trained', args.embed_model + '_Extractor')\n","        feature_encoder.load_state_dict(torch.load(model_path, map_location='cuda:0'))\n","        self.feature_encoder = feature_encoder\n","        self.feature_encoder.eval()\n","\n","\n","        print('##BUILDING CONNECTION MATRIX')\n","        self.degrees, self.connections, self.e1_degrees = self.build_connection(num_ents, pad_id, self.symbol2id, self.ent2id, max_=args.max_neighbor)\n","\n","        print('################ DEFINE ZSL MODEL AND LOSS FUNCTION ... ##############')\n","        self.zsl_model = DeViSE(self.input_dims, args.hidden_dims, args.output_dims, args.p).cuda()\n","        self.loss_fn = nn.MSELoss()\n","        self.optimizer_tag = optim.Adam(self.zsl_model.parameters(), lr=args.lr, weight_decay=args.wds)\n","        print('using {} as criterion'.format(self.loss_fn))\n","\n","\n","\n","\n","\n","\n","\n","    def read_embed(self):\n","        symbol_id = json.load(open(os.path.join(self.args.data_path, 'Embed_used', args.embed_model + '2id')))\n","        embeddings = np.load(os.path.join(self.args.data_path, 'Embed_used', args.embed_model + '.npz'))['arr_0']\n","        symbol2id = symbol_id\n","        symbol2vec = embeddings\n","        return symbol2id, symbol2vec\n","\n","    #  build neighbor connection\n","    def build_connection(self, num_ents, pad_id, symbol2id, ent2id, max_=100):\n","\n","        connections = (np.ones((num_ents, max_, 2)) * pad_id).astype(int)\n","        e1_rele2 = defaultdict(list)\n","        e1_degrees = defaultdict(int)\n","        # rel_list = list()\n","        with open(os.path.join(self.args.data_path, 'path_graph')) as f:\n","            lines = f.readlines()\n","            for line in tqdm(lines):\n","                e1, rel, e2 = line.rstrip().split()\n","                e1_rele2[e1].append((symbol2id[rel], symbol2id[e2]))\n","                e1_rele2[e2].append((symbol2id[rel], symbol2id[e1]))\n","\n","        # print(\"path graph relations:\", len(set(rel_list)))\n","        degrees = {}\n","        for ent, id_ in ent2id.items():\n","            neighbors = e1_rele2[ent]\n","            if len(neighbors) > max_:\n","                neighbors = neighbors[:max_]\n","            # degrees.append(len(neighbors))\n","            degrees[ent] = len(neighbors)\n","            e1_degrees[id_] = len(neighbors)  # add one for self conn\n","            for idx, _ in enumerate(neighbors):\n","                connections[id_, idx, 0] = _[0]\n","                connections[id_, idx, 1] = _[1]\n","\n","\n","        return degrees, connections, e1_degrees\n","\n","    def get_meta(self, left, right, connections, e1_degrees):\n","        left_connections = Variable(\n","            torch.LongTensor(np.stack([connections[_, :, :] for _ in left], axis=0))).cuda()\n","        left_degrees = Variable(torch.FloatTensor([e1_degrees[_] for _ in left])).cuda()\n","\n","        right_connections = Variable(\n","            torch.LongTensor(np.stack([connections[_, :, :] for _ in right], axis=0))).cuda()\n","        right_degrees = Variable(torch.FloatTensor([e1_degrees[_] for _ in right])).cuda()\n","\n","        return (left_connections, left_degrees, right_connections, right_degrees)\n","\n","\n","\n","    def train(self):\n","\n","\n","        print('\\n############ START TRAINING ... ############')\n","        losses = deque([], args.loss_every)\n","\n","\n","        train_data = load_train_data(self.args, self.train_tasks, self.symbol2id, self.ent2id,\n","                                           self.e1rel_e2, self.rel2id, self.rela2label, self.rela_matrix)\n","\n","\n","\n","        for epoch in range(1, (args.train_times+1)):\n","            self.zsl_model.train()\n","            \n","            rel_sem, rel_sem_neg, query, query_left, query_right, false, false_left, false_right, labels = train_data.__next__()\n","\n","            rel_sem = Variable(torch.FloatTensor(rel_sem)).cuda()\n","\n","\n","            # encoding (h, t) pair\n","            query_meta = self.get_meta(query_left, query_right, self.connections, self.e1_degrees)\n","            query = Variable(torch.LongTensor(query)).cuda()\n","\n","            entity_pair_vector, _ = self.feature_encoder(query, query, query_meta, query_meta)\n","\n","            self.zsl_model.zero_grad()\n","\n","            rel_sem_mapped = self.zsl_model(rel_sem)\n","            loss = self.loss_fn(entity_pair_vector, rel_sem_mapped)\n","\n","\n","            loss.backward()\n","            self.optimizer_tag.step()\n","\n","\n","            losses.append(loss.item())\n","\n","\n","\n","            if epoch % args.loss_every == 0:\n","\n","                print(\"Epoch: %d, loss: %.3f\" % (epoch, np.mean(losses)))\n","\n","\n","            if epoch >= 1000 and epoch % args.eval_every == 0:\n","                self.test(epoch)\n","                # self.save_model()\n","\n","    def test(self, epoch=0):\n","        self.zsl_model.eval()\n","\n","        # test_candidates = json.load(open(os.path.join(self.args.data_path, \"test_candidates_sub_10.json\")))\n","        test_candidates = json.load(open(os.path.join(self.args.data_path, \"test_candidates.json\")))\n","\n","        hits10 = []\n","        hits5 = []\n","        hits1 = []\n","        mrr = []\n","\n","        for query_ in sorted(test_candidates.keys()):\n","\n","\n","            hits10_ = []\n","            hits5_ = []\n","            hits1_ = []\n","            mrr_ = []\n","\n","            rel_sem = self.rela_matrix[self.rel2id[query_]]\n","            rel_sem = np.expand_dims(rel_sem, axis=0)\n","            rel_sem = Variable(torch.FloatTensor(rel_sem)).cuda()\n","\n","            rel_sem_mapped = self.zsl_model(rel_sem)\n","            rel_sem_mapped.detach()\n","            rel_sem_mapped = rel_sem_mapped.data.cpu().numpy()\n","\n","            for e1_rel, tail_candidates in test_candidates[query_].items():\n","                if args.dataset == \"NELL\":\n","                    head, rela, _ = e1_rel.split('\\t')\n","                elif args.dataset == \"Wiki\":\n","                    head, rela = e1_rel.split('\\t')\n","                else:\n","                    print('The Dataset is not supported, please check')\n","\n","                true = tail_candidates[0]\n","                query_pairs = []\n","                if head not in self.symbol2id or true not in self.symbol2id:\n","                    continue\n","                query_pairs.append([self.symbol2id[head], self.symbol2id[true]])\n","\n","                query_left = []\n","                query_right = []\n","                query_left.append(self.ent2id[head])\n","                query_right.append(self.ent2id[true])\n","\n","                for tail in tail_candidates[1:]:\n","                    if tail not in self.symbol2id:\n","                        continue\n","                    query_pairs.append([self.symbol2id[head], self.symbol2id[tail]])\n","                    query_left.append(self.ent2id[head])\n","                    query_right.append(self.ent2id[tail])\n","\n","                query = Variable(torch.LongTensor(query_pairs)).cuda()\n","\n","                query_meta = self.get_meta(query_left, query_right, self.connections, self.e1_degrees)\n","                candidate_vecs, _ = self.feature_encoder(query, query, query_meta, query_meta)\n","                candidate_vecs.detach()\n","                candidate_vecs = candidate_vecs.data.cpu().numpy()\n","                scores = cosine_similarity(candidate_vecs, rel_sem_mapped)\n","                scores = np.squeeze(scores, axis=1)\n","\n","                assert scores.shape == (len(query_pairs),)\n","\n","                sort = list(np.argsort(scores))[::-1]  # ascending -> descending\n","                rank = sort.index(0) + 1\n","                if rank <= 10:\n","                    hits10.append(1.0)\n","                    hits10_.append(1.0)\n","                else:\n","                    hits10.append(0.0)\n","                    hits10_.append(0.0)\n","                if rank <= 5:\n","                    hits5.append(1.0)\n","                    hits5_.append(1.0)\n","                else:\n","                    hits5.append(0.0)\n","                    hits5_.append(0.0)\n","                if rank <= 1:\n","                    hits1.append(1.0)\n","                    hits1_.append(1.0)\n","                else:\n","                    hits1.append(0.0)\n","                    hits1_.append(0.0)\n","                mrr.append(1.0 / rank)\n","                mrr_.append(1.0 / rank)\n","\n","\n","\n","        print('\\n############   ' + 'TEST' + ' ' + str(epoch) + '    #############')\n","        print('HITS10: {:.3f}, HITS5: {:.3f}, HITS1: {:.3f}, MAP: {:.3f}'.format(np.mean(hits10),\n","                                                                                 np.mean(hits5),\n","                                                                                 np.mean(hits1),\n","                                                                                 np.mean(mrr)))\n","        print('###################################')"],"metadata":{"id":"4iyRniD1DwVx","executionInfo":{"status":"ok","timestamp":1666193887003,"user_tz":-480,"elapsed":5,"user":{"displayName":"黄雨峰","userId":"10355830370640481145"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["run = Runner(args)\n","run.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZAI2T3mCD7MV","executionInfo":{"status":"ok","timestamp":1665415006482,"user_tz":-480,"elapsed":5103116,"user":{"displayName":"黄雨峰","userId":"10355830370640481145"}},"outputId":"92f05d21-bcc4-42ae-af15-6472e3466c21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["############## LOADING .... ###################\n","##LOADING ENTITY##\n","##LOADING CANDIDATES ENTITIES##\n","##LOADING SYMBOL ID AND SYMBOL EMBEDDING\n","num symbols: 65748\n","Load Pretrained Feature Encoder!\n","##BUILDING CONNECTION MATRIX\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 181053/181053 [00:00<00:00, 534884.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["################ DEFINE ZSL MODEL AND LOSS FUNCTION ... ##############\n","using MSELoss() as criterion\n","\n","############ START TRAINING ... ############\n","##LOADING CANDIDATES\n","Epoch: 50, loss: 1.498\n","Epoch: 100, loss: 1.092\n","Epoch: 150, loss: 0.816\n","Epoch: 200, loss: 0.722\n","Epoch: 250, loss: 0.693\n","Epoch: 300, loss: 0.677\n","Epoch: 350, loss: 0.676\n","Epoch: 400, loss: 0.644\n","Epoch: 450, loss: 0.653\n","Epoch: 500, loss: 0.637\n","Epoch: 550, loss: 0.640\n","Epoch: 600, loss: 0.642\n","Epoch: 650, loss: 0.628\n","Epoch: 700, loss: 0.633\n","Epoch: 750, loss: 0.628\n","Epoch: 800, loss: 0.625\n","Epoch: 850, loss: 0.619\n","Epoch: 900, loss: 0.606\n","Epoch: 950, loss: 0.615\n","Epoch: 1000, loss: 0.618\n","\n","############   TEST 1000    #############\n","HITS10: 0.346, HITS5: 0.285, HITS1: 0.153, MAP: 0.220\n","###################################\n","Epoch: 1050, loss: 0.616\n","Epoch: 1100, loss: 0.626\n","Epoch: 1150, loss: 0.636\n","Epoch: 1200, loss: 0.618\n","Epoch: 1250, loss: 0.608\n","Epoch: 1300, loss: 0.615\n","Epoch: 1350, loss: 0.606\n","Epoch: 1400, loss: 0.608\n","Epoch: 1450, loss: 0.616\n","Epoch: 1500, loss: 0.614\n","\n","############   TEST 1500    #############\n","HITS10: 0.344, HITS5: 0.286, HITS1: 0.154, MAP: 0.221\n","###################################\n","Epoch: 1550, loss: 0.598\n","Epoch: 1600, loss: 0.610\n","Epoch: 1650, loss: 0.601\n","Epoch: 1700, loss: 0.618\n","Epoch: 1750, loss: 0.609\n","Epoch: 1800, loss: 0.610\n","Epoch: 1850, loss: 0.617\n","Epoch: 1900, loss: 0.614\n","Epoch: 1950, loss: 0.616\n","Epoch: 2000, loss: 0.621\n","\n","############   TEST 2000    #############\n","HITS10: 0.345, HITS5: 0.285, HITS1: 0.153, MAP: 0.220\n","###################################\n","Epoch: 2050, loss: 0.609\n","Epoch: 2100, loss: 0.604\n","Epoch: 2150, loss: 0.612\n","Epoch: 2200, loss: 0.613\n","Epoch: 2250, loss: 0.612\n","Epoch: 2300, loss: 0.616\n","Epoch: 2350, loss: 0.598\n","Epoch: 2400, loss: 0.613\n","Epoch: 2450, loss: 0.604\n","Epoch: 2500, loss: 0.611\n","\n","############   TEST 2500    #############\n","HITS10: 0.353, HITS5: 0.291, HITS1: 0.154, MAP: 0.223\n","###################################\n","Epoch: 2550, loss: 0.600\n","Epoch: 2600, loss: 0.602\n","Epoch: 2650, loss: 0.590\n","Epoch: 2700, loss: 0.603\n","Epoch: 2750, loss: 0.614\n","Epoch: 2800, loss: 0.603\n","Epoch: 2850, loss: 0.603\n","Epoch: 2900, loss: 0.608\n","Epoch: 2950, loss: 0.595\n","Epoch: 3000, loss: 0.610\n","\n","############   TEST 3000    #############\n","HITS10: 0.348, HITS5: 0.286, HITS1: 0.154, MAP: 0.221\n","###################################\n","Epoch: 3050, loss: 0.602\n","Epoch: 3100, loss: 0.610\n","Epoch: 3150, loss: 0.606\n","Epoch: 3200, loss: 0.602\n","Epoch: 3250, loss: 0.591\n","Epoch: 3300, loss: 0.597\n","Epoch: 3350, loss: 0.606\n","Epoch: 3400, loss: 0.604\n","Epoch: 3450, loss: 0.597\n","Epoch: 3500, loss: 0.607\n","\n","############   TEST 3500    #############\n","HITS10: 0.351, HITS5: 0.293, HITS1: 0.156, MAP: 0.225\n","###################################\n","Epoch: 3550, loss: 0.605\n","Epoch: 3600, loss: 0.595\n","Epoch: 3650, loss: 0.605\n","Epoch: 3700, loss: 0.592\n","Epoch: 3750, loss: 0.601\n","Epoch: 3800, loss: 0.598\n","Epoch: 3850, loss: 0.609\n","Epoch: 3900, loss: 0.600\n","Epoch: 3950, loss: 0.604\n","Epoch: 4000, loss: 0.611\n","\n","############   TEST 4000    #############\n","HITS10: 0.350, HITS5: 0.291, HITS1: 0.159, MAP: 0.225\n","###################################\n","Epoch: 4050, loss: 0.605\n","Epoch: 4100, loss: 0.596\n","Epoch: 4150, loss: 0.597\n","Epoch: 4200, loss: 0.612\n","Epoch: 4250, loss: 0.592\n","Epoch: 4300, loss: 0.605\n","Epoch: 4350, loss: 0.610\n","Epoch: 4400, loss: 0.602\n","Epoch: 4450, loss: 0.612\n","Epoch: 4500, loss: 0.591\n","\n","############   TEST 4500    #############\n","HITS10: 0.347, HITS5: 0.286, HITS1: 0.156, MAP: 0.221\n","###################################\n","Epoch: 4550, loss: 0.600\n","Epoch: 4600, loss: 0.599\n","Epoch: 4650, loss: 0.598\n","Epoch: 4700, loss: 0.586\n","Epoch: 4750, loss: 0.583\n","Epoch: 4800, loss: 0.584\n","Epoch: 4850, loss: 0.589\n","Epoch: 4900, loss: 0.599\n","Epoch: 4950, loss: 0.588\n","Epoch: 5000, loss: 0.601\n","\n","############   TEST 5000    #############\n","HITS10: 0.347, HITS5: 0.289, HITS1: 0.155, MAP: 0.222\n","###################################\n","Epoch: 5050, loss: 0.597\n","Epoch: 5100, loss: 0.590\n","Epoch: 5150, loss: 0.598\n","Epoch: 5200, loss: 0.598\n","Epoch: 5250, loss: 0.600\n","Epoch: 5300, loss: 0.604\n","Epoch: 5350, loss: 0.612\n","Epoch: 5400, loss: 0.598\n","Epoch: 5450, loss: 0.584\n","Epoch: 5500, loss: 0.594\n","\n","############   TEST 5500    #############\n","HITS10: 0.346, HITS5: 0.286, HITS1: 0.154, MAP: 0.221\n","###################################\n","Epoch: 5550, loss: 0.596\n","Epoch: 5600, loss: 0.604\n","Epoch: 5650, loss: 0.590\n","Epoch: 5700, loss: 0.596\n","Epoch: 5750, loss: 0.587\n","Epoch: 5800, loss: 0.599\n","Epoch: 5850, loss: 0.602\n","Epoch: 5900, loss: 0.609\n","Epoch: 5950, loss: 0.602\n","Epoch: 6000, loss: 0.595\n","\n","############   TEST 6000    #############\n","HITS10: 0.349, HITS5: 0.289, HITS1: 0.155, MAP: 0.223\n","###################################\n","Epoch: 6050, loss: 0.593\n","Epoch: 6100, loss: 0.593\n","Epoch: 6150, loss: 0.590\n","Epoch: 6200, loss: 0.594\n","Epoch: 6250, loss: 0.592\n","Epoch: 6300, loss: 0.603\n","Epoch: 6350, loss: 0.588\n","Epoch: 6400, loss: 0.590\n","Epoch: 6450, loss: 0.590\n","Epoch: 6500, loss: 0.593\n","\n","############   TEST 6500    #############\n","HITS10: 0.346, HITS5: 0.291, HITS1: 0.156, MAP: 0.223\n","###################################\n","Epoch: 6550, loss: 0.592\n","Epoch: 6600, loss: 0.596\n","Epoch: 6650, loss: 0.594\n","Epoch: 6700, loss: 0.600\n","Epoch: 6750, loss: 0.583\n","Epoch: 6800, loss: 0.591\n","Epoch: 6850, loss: 0.589\n","Epoch: 6900, loss: 0.592\n","Epoch: 6950, loss: 0.595\n","Epoch: 7000, loss: 0.577\n","\n","############   TEST 7000    #############\n","HITS10: 0.343, HITS5: 0.285, HITS1: 0.152, MAP: 0.219\n","###################################\n"]}]},{"cell_type":"markdown","source":["**Parameters in other Settings**\n","\n","\n","---\n","\n","*   **run DeViSE on NELL with \"RDFS+literals\"**\n","\n","------HYPERPARAMETERS-------\n","data_dir: /content/drive/MyDrive/ISWC_demo/ZS_KGC/data;\n","dataset: NELL;\n","embed_model: TransE;\n","max_neighbor: 50;\n","embed_dim: 100;\n","ep_dim: 200;\n","batch_size: 64;\n","batch_rela_num: 4;\n","train_times: 7000;\n","loss_every: 50;\n","eval_every: 500;\n","p: 0.5;\n","lr: 0.001;\n","wds: 1e-05;\n","manual_seed: 12345;\n","semantic_type: rdfs_text;\n","hidden_dims: 400;\n","\n","*   **run DeViSE on Wiki with \"RDFS\"**\n","\n","------HYPERPARAMETERS-------\n","data_dir: /content/drive/MyDrive/ISWC_demo/ZS_KGC/data;\n","dataset: Wiki;\n","embed_model: TransE;\n","max_neighbor: 50;\n","embed_dim: 50;\n","ep_dim: 100;\n","batch_size: 64;\n","batch_rela_num: 8;\n","train_times: 7000;\n","loss_every: 50;\n","eval_every: 500;\n","p: 0.5;\n","lr: 0.001;\n","wds: 1e-05;\n","manual_seed: 12345;\n","semantic_type: rdfs;\n","hidden_dims: 200;\n","\n","*   **run DeViSE on Wiki with \"RDFS+literals\"**\n","\n","------HYPERPARAMETERS-------\n","data_dir: /content/drive/MyDrive/ISWC_demo/ZS_KGC/data;\n","dataset: Wiki;\n","embed_model: TransE;\n","max_neighbor: 50;\n","embed_dim: 50;\n","ep_dim: 100;\n","batch_size: 64;\n","batch_rela_num: 8;\n","train_times: 7000;\n","loss_every: 50;\n","eval_every: 500;\n","p: 0.5;\n","lr: 0.001;\n","wds: 1e-05;\n","manual_seed: 12345;\n","semantic_type: rdfs_text;\n","hidden_dims: 200;\n","\n","\n","---\n","\n","\n","**The best parameter of hidden_dim for NELL**\n","\n","is set to 300 for rdfs, rdfs_hie, rdfs_cons and text, is set to 400 for rdfs_text\n","\n","**The best parameter of hidden_dim for Wiki**\n","\n","is set to 200 for rdfs, rdfs_hie, rdfs_cons, text and rdfs_text"],"metadata":{"id":"pjfTmzHgvKSL"}}]}